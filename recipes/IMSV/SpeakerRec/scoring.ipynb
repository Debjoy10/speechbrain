{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Metrics from scores.txt files saved in verification run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of verification pairs = 150\n"
     ]
    }
   ],
   "source": [
    "res = '/media/absp/4E897AE46CE3ABFA/IMSV/results/results/imsv/speaker_verification_ecapa_big/scores.txt'\n",
    "\n",
    "with open(res) as f:\n",
    "    scores = [line.split(' ') for line in f.readlines()]\n",
    "print(\"Number of verification pairs = {}\".format(len(scores)))\n",
    "\n",
    "# Collecting positive and negative scores\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "\n",
    "for (enrol_id, test_id, lab_pair, score) in scores:\n",
    "    if int(lab_pair) == 1:\n",
    "        positive_scores.append(float(score))\n",
    "    else:\n",
    "        negative_scores.append(float(score))\n",
    "\n",
    "import torch\n",
    "from tqdm.contrib import tqdm\n",
    "from speechbrain.utils.metric_stats import EER, minDCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER(%)=0.0\n",
      "Th = 8.411\n"
     ]
    }
   ],
   "source": [
    "# EER\n",
    "eer, th = EER(torch.tensor(positive_scores), torch.tensor(negative_scores))\n",
    "print(\"EER(%)={}\".format(eer * 100))\n",
    "print(\"Th = {0:.3f}\".format(th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minDCF=0.0\n",
      "Th = 8.411\n"
     ]
    }
   ],
   "source": [
    "# Min-DCF\n",
    "min_dcf, th = minDCF(torch.tensor(positive_scores[:10000]), torch.tensor(negative_scores[:10000]))\n",
    "print(\"minDCF={}\".format(min_dcf * 100))\n",
    "print(\"Th = {0:.3f}\".format(th))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "### Ensembling Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(files, fname, facts = None):\n",
    "    def extract_tups(file):\n",
    "        with open(file) as f:\n",
    "            tups = [[z.strip() for i, z in enumerate(x.split(' '))] for x in f.readlines()]\n",
    "            scores = np.array([float(t[3]) for t in tups])\n",
    "        return tups, scores\n",
    "    def tup_to_txt(tup, file = fname):\n",
    "        tup = [' '.join(t)+'\\n' for t in tup]\n",
    "        with open(file, 'w') as f:\n",
    "            f.writelines(tup)\n",
    "    s_all = None\n",
    "    if facts is None:\n",
    "        facts = [1/len(files) for _ in files]\n",
    "    for i, f in enumerate(files):\n",
    "        t, s = extract_tups(f)\n",
    "        if s_all is not None:\n",
    "            s_all += s*facts[i]\n",
    "        else:\n",
    "            s_all = s*facts[i]\n",
    "    tf = []\n",
    "    for i, ti in enumerate(t):\n",
    "        tx = copy.deepcopy(ti)\n",
    "        tx[-1] = str(s_all[i])\n",
    "        tf.append(tx)\n",
    "    tup_to_txt(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "globfs = \"/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/*/scores.txt\"\n",
    "files = glob.glob(globfs)\n",
    "ensemble(files, 'avgd.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in files if 'ecapa' in f]\n",
    "ensemble(files, 'avgd_ecapa_only.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA/scores.txt']\n",
    "ensemble(files, 'avgd_ecapa_closed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_vggvox/scores.txt','/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_vggvox_DA/scores.txt']\n",
    "ensemble(files, 'avgd_closed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_vggvox/scores.txt','/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_vggvox_DA/scores.txt']\n",
    "ensemble(files, 'avgd_closed_DiffEns.txt', [0.37474135, 0.03572364, 0.23668878, 0.35284623])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA/scores.txt']\n",
    "ensemble(files, 'avgd_ecapa_closed_DiffEns.txt', [0.74896971, 0.25103029])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big/scores.txt',\n",
    " '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained/scores.txt',\n",
    " '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained_pretrained/scores.txt',\n",
    " '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA/scores.txt',\n",
    " '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA_pretrained/scores.txt']\n",
    "weights = np.array([5.1428, 3.0488, 3.3043, 2.7601, 2.8076])\n",
    "weights = np.exp(-weights)\n",
    "weights = weights / sum(weights)\n",
    "ensemble(files, 'scores/weightedavgd_expacc.txt', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_big_trained/scores.txt',\n",
    " '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_test/speaker_verification_ecapa_DA/scores.txt']\n",
    "weights = np.array([3.0488, 2.7601])\n",
    "weights = np.exp(-weights)\n",
    "weights = weights / sum(weights)\n",
    "ensemble(files, 'scores/weightedavgd_expacc_closed.txt', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "# global optimization to find coefficients for weighted ensemble on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution\n",
    "from speechbrain.utils.metric_stats import EER, minDCF\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(preds, weights):\n",
    "    return sum([p*w for p, w in zip(preds, weights)])\n",
    "    \n",
    "# # evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(preds, weights, lpf):\n",
    "    scores = ensemble_predictions(preds, weights)\n",
    "    # Collecting positive and negative scores\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "    for i, score in enumerate(scores):\n",
    "        lab_pair = lpf[i]\n",
    "        if int(lab_pair) == 1:\n",
    "            positive_scores.append(score)\n",
    "        else:\n",
    "            negative_scores.append(score)\n",
    "    eer, th = EER(torch.tensor(positive_scores), torch.tensor(negative_scores))\n",
    "    return 1-eer\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, preds, lpf):\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "    # calculate error rate\n",
    "    return 1.0 - evaluate_ensemble(preds, weights, lpf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading /media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_verify/speaker_verification_ecapa_big_trained/scores.txt\n",
      "reading /media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_verify/speaker_verification_ecapa_DA/scores.txt\n",
      "Equal Weights Score: 0.980\n"
     ]
    }
   ],
   "source": [
    "files = ['/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_verify/speaker_verification_ecapa_big_trained/scores.txt', '/media/absp/4E897AE46CE3ABFA/IMSV/results/imsv_verify/speaker_verification_ecapa_DA/scores.txt']\n",
    "n_members = len(files)\n",
    "scores_all = []\n",
    "for res in files:\n",
    "    print(\"reading {}\".format(res))\n",
    "    with open(res) as f:\n",
    "        scores = [float(line.split(' ')[-1]) for line in f.readlines()]\n",
    "    scores_all.append(np.array(scores))\n",
    "with open(res) as f:\n",
    "    lpf = [int(line.split(' ')[-2]) for line in f.readlines()]\n",
    "\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(scores_all, weights, lpf)\n",
    "print('Equal Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights: [0.74896971 0.25103029]\n",
      "Optimized Weights Score: 0.995\n"
     ]
    }
   ],
   "source": [
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0)  for _ in range(n_members)]\n",
    "# arguments to the loss function\n",
    "search_arg = (scores_all, lpf)\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(func = loss_function, bounds = bound_w, args = search_arg, maxiter=1000, tol=1e-7)\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(scores_all, weights, lpf)\n",
    "print('Optimized Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debjoy",
   "language": "python",
   "name": "debjoy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
